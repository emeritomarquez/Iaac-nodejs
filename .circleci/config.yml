#
# Use the latest 2.1 version of CircleCI pipeline process engine. See: https://circleci.com/docs/2.0/configuration-reference
#
version: 2.1
# Use a package of configuration called an orb.
orbs:
  # Choose either one of the orbs below
  # Declare a dependency on the welcome-orb
  # welcome: circleci/welcome-orb@0.4.1
  aws-cli: circleci/aws-cli@2.0.3
# Orchestrate or schedule a set of jobs

commands:
  # Exercise: Reusable Job Code
  print_pipeline_id:
    parameters:
      id:
        type: string
    steps:
      - run: echo << parameters.id >>

  # Exercise - Rollback
  destroy_environment:
    steps:
      - run:
          name: Destroy environment
          when: on_fail
          # ${CIRCLE_WORKFLOW_ID} is a Built-in environment variable
          # ${CIRCLE_WORKFLOW_ID:0:5} takes the first 5 chars of the variable CIRCLE_CI_WORKFLOW_ID
          command: |
            aws cloudformation delete-stack --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:5}

jobs:
  # Exercise: Creating a Simple Workflow
  # Exercise: Environment Variables
  # Exercise: Reusable Job Code
  print_greetings:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - print_pipeline_id:
          id: << pipeline.id >>
      - run: echo HELLO
      - run: echo WORLD
      - run: echo $_env_name
      - run: ls -al

  # Exercise: Infrastructure Creation
  # Exercise - Rollback
  create_infrastructure:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
              --template-file template.yml \
              --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:5} \
              --region us-west-2
      - run: ls -al
      # Fail the job intentionally to simulate an error.
      # Uncomment the line below if you want to fail the current step
      #- run: return 1
      #- destroy_environment

  # Exercise: Sharing Files (Job names may have changed)
#  upload_file:
#    docker:
#      - image: circleci/node:13.8.0
#    steps:
#      - run: echo "[hosts]" > ~/inventory.txt
#      - persist_to_workspace:
#          root: ~/
#          paths:
#            - inventory.txt


  create_inventory_file:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Install tar utility
          command: |
            yum install -y tar gzip
      - run: echo "[hosts]" > ~/inventory.txt
      - run:
          name: Create inventory file
          command: |
            aws ec2 describe-instances \
              --query 'Reservations[*].Instances[*].PublicIpAddress' \
              --output text >> ~/inventory.txt
      - run: cat ~/inventory.txt
      - run: touch ~/inventory1.txt
      - run: ls -al
      - run: cat inventory2.py
      - run: python inventory2.py ~/inventory.txt ~/inventory1.txt
      - persist_to_workspace:
          root: ~/
          paths:
            - inventory1.txt

#  download_file:
#    docker:
#      - image: circleci/node:13.8.0
#    steps:
#      - attach_workspace:
#          at: ~/
#      - run: cat ~/inventory1.txt

  # Exercise: Config and Deployment
  configure_infrastructure:
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - run: touch ~/inventory1.txt
      - attach_workspace:
          at: ~/
      - run: cat ~/inventory1.txt
      - add_ssh_keys:
          fingerprints: ["24:43:5c:cd:04:ca:0d:b4:a9:a6:72:1d:2d:73:56:31"] # You can get this ID in the section where you registered the SSH Key
      - run:
          name: Install dependencies
          command: |
            # install the dependencies needed for your playbook
            apk add --update ansible
      - run: ls -al /
      - run: ls -al ~
      - run: ls -al
      - run: cat ~/inventory1.txt
      - run: sleep 60 
#      - run: ansible -vvv asdf | grep ansible.cfg
      - run: cat /root/project/ansible.cfg
      - run: ls -al /root/project
#      - run: cp ~/inventory1.txt /etc/ansible/hosts
#      - run: sudo chmod 666 /etc/ansible/ansible.cfg
#      - run: sudo sed -i 's/#inventory/inventory/g' /etc/ansible/ansible.cfg
      - run:
          name: Configure server
          command: |
            ansible-playbook -v -i ~/inventory1.txt --private-key=ec2-emerito.pem main.yml
      - persist_to_workspace:
          root: ~/
          paths:
            - inventory1.txt

# Exercise: Smoke Testing
  smoke_test:
    docker:
      - image: python:3.7-alpine3.11 
#      - image: amazon/aws-cli
    steps:
      - checkout
      - run: apk add --update curl
      - attach_workspace:
          at: ~/
      - run: cat  ~/inventory1.txt

#Copy a file to an environment variable
#!/bin/bash
#value=$(<config.txt)
#echo "$value"
#      - run: ls -al /
#      - run: ls -al ~
#      - run: ls -al ~/project
#      - run: ls -al
#      - run: echo "#!/usr/bin/python" > dst-ip.py
#      - run: echo "import sys" >> dst-ip.py
#      - run: echo "f = open(sys.argv[1], 'r')" >> dst-ip.py
#      - run: echo "lines = f.readlines()" >> dst-ip.py
#      - run: echo "f.close()" >> dst-ip.py
#      - run: echo "rows = []" >> dst-ip.py
#      - run: echo "for line in lines:" >> dst-ip.py
#      - run: echo "    if line != \'[hosts]\':" >> dst-ip.py
#      - run: echo "        rows.append(line)" >> dst-ip.py
#      - run: echo "print(rows)" >> dst-ip.py
#      - run: echo "g = open(sys.argv[2], 'w+')" >> dst-ip.py
#      - run: echo "for element in rows:" >> dst-ip.py
#      - run: echo "    g.write(element)" >> dst-ip.py
#      - run: echo "g.close()" >> dst-ip.py
#      - run: cat dst-ip.py
#      - run: python dst-ip.py ~/inventory1.txt ~/dst-ip.txt

      - run:
          name: smoke test
          command: |
            ls -al /
            ls -al ~/
            ls -al
            ls -al ~/project
            python inventory3.py ~/inventory1.txt ~/inventory3.txt
            URL=$(cat ~/inventory3.txt)
            od -c ~/inventory1.txt
            od -c ~/inventory3.txt
            # Test if website exists
            if curl -s --head ${URL}
              then
                return 0
            else
                return 1
            fi

      - destroy_environment
#        when: on_fail

  # Exercise: Promote to Production - Job 1
  # Prerequisite:
  # 1. An S3 bucket (say `mybucket644752792305`) with a sample index.html created manually in your AWS console.
  # 2. Enable the Static website hosting in that bucket.
  # 3. Run the command below to create a CloudFront Distribution that will connect to the existing bucket.
  # Note that the `--parameter-overrides` let you specify a value that override parameter value in the cloudfront.yml template file.
  # We are assuming that the `PipelineID` parameter represents the bucket ID.

  # aws cloudformation deploy \
  # --template-file cloudfront.yml \
  # --stack-name production-distro \
  # --parameter-overrides PipelineID="mybucket644752792305" \ # Name of the S3 bucket you created manually.

  # Job 1
  # Executes the bucket.yml - Deploy an S3 bucket, and interface with that bucket to synchronize the files between local and the bucket.
  # Note that the `--parameter-overrides` let you specify a value that override parameter value in the bucket.yml template file.
#  create_and_deploy_front_end:
#    docker:
#      - image: amazon/aws-cli
#    steps:
#      - checkout
#      - run:
#          name: Execute bucket.yml - Create Cloudformation Stack
#          command: |
#            aws cloudformation deploy \
#            --template-file bucket.yml \
#            --stack-name stack-create-bucket-${CIRCLE_WORKFLOW_ID:0:7} \
#            --parameter-overrides MyBucketName="mybucket-${CIRCLE_WORKFLOW_ID:0:7}"
      # Uncomment the step below if yoou wish to upload all contents of the current directory to the S3 bucket
      # - run: aws s3 sync . s3://mybucket-${CIRCLE_WORKFLOW_ID:0:7} --delete

  # Exercise: Promote to Production - Job 2
  # Fetch and save the pipeline ID (bucket ID) responsible for the last release.
#  get_last_deployment_id:
#    docker:
#      - image: amazon/aws-cli
#    steps:
#      - checkout
#      - run: yum install -y tar gzip
#      - run:
#          name: Fetch and save the old pipeline ID (bucket name) responsible for the last release.
#          command: |
#            aws cloudformation \
#            list-exports --query "Exports[?Name==\`PipelineID\`].Value" \
#            --no-paginate --output text > ~/textfile.txt
#      - persist_to_workspace:
#          root: ~/
#          paths:
#            - textfile.txt

  # Exercise: Promote to Production - Job 3
  # Executes the cloudfront.yml template that will modify the existing CloudFront Distribution, change its target from the old bucket to the new bucket - `mybucket-${CIRCLE_WORKFLOW_ID:0:7}`.
  # Notice here we use the stack name `production-distro` which is the same name we used while deploying to the S3 bucket manually.
#  promote_to_production:
#    docker:
#      - image: amazon/aws-cli
#    steps:
#      - checkout
#      - run:
#          name: Execute cloudfront.yml
#          command: |
#            aws cloudformation deploy \
#            --template-file cloudfront.yml \
#            --stack-name production-distro \
#            --parameter-overrides PipelineID="mybucket-${CIRCLE_WORKFLOW_ID:0:7}"

  # Exercise: Promote to Production - Job 4
  # Destroy the previous production version's S3 bucket and CloudFormation stack.
#  clean_up_old_front_end:
#    docker:
#      - image: amazon/aws-cli
#    steps:
#      - checkout
#      - run: yum install -y tar gzip
#      - attach_workspace:
#          at: ~/
#      - run:
#          name: Destroy the previous production version's S3 bucket and CloudFormation stack.
          # Use $OldBucketID environment variable or mybucket644752792305 below.
          # Similarly, you can create and use $OldStackID environment variable in place of production-distro
#          command: |
#            export OldBucketID=$(cat ~/textfile.txt)
#            aws s3 rm "s3://${OldBucketID}" --recursive
          #  aws cloudformation delete-stack --stack-name production-distro
          #  aws cloudformation delete-stack --stack-name stack-create-bucket-${CIRCLE_WORKFLOW_ID:0:7}
          #  aws cloudformation delete-stack --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:5}


#Workflow my_workflow
workflows:
  # Name the workflow "welcome"
  my_workflow:
    # Run the welcome/run job in its own container
    jobs:
      # - welcome/run
      - print_greetings
      - create_infrastructure
#      - upload_file:
#          requires:
#            - create_infrastructure
      - create_inventory_file:
          requires:
#            - upload_file
            - create_infrastructure
#      - download_file:
#          requires:
#            - create_inventory_file
      - configure_infrastructure:
          requires:
            - create_inventory_file
      - smoke_test:
          requires:
            - configure_infrastructure
#      - rollback:
#         requires:
#           - smoke_test

#      - create_and_deploy_front_end
#      - promote_to_production:
#          requires:
#            - create_and_deploy_front_end
#      - get_last_deployment_id
#      - clean_up_old_front_end:
#          requires:
#            - get_last_deployment_id
#            - promote_to_production
